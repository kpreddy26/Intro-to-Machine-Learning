# -*- coding: utf-8 -*-
"""Logistic Regression of Purchase_logistic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p6pHDLKcm3QvTjBu9UDZuVJRf5wh4Bl1

Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    roc_curve,
    roc_auc_score,
    mean_squared_error
)
from sklearn.neighbors import KNeighborsClassifier

"""Load The Dataset"""

from google.colab import files
import matplotlib.pyplot as plt

df = pd.read_csv("Purchase_Logistic.csv")

df.head()

"""Categorical value exist ,need to map with binary value"""

df["Gender"] = df["Gender"].map({"Male": 0, "Female": 1})

df.info()

df.shape#small dataset

df.columns

"""is there any missing value"""

print("\nMissing values per column:")
print(df.isnull().sum())

"""No missing value"""

df[df.duplicated(keep='first')]

print(df.describe())

print(df["Purchased"].value_counts())

"""Imbalance Dataset

"""

print(" ID is unique:", df["User ID"].is_unique)

"""ID is unique so we remove this column for our training model

Visualise the Features
"""

plt.figure()
plt.scatter(df["Age"], df["Purchased"])
plt.xlabel("Age")
plt.ylabel("Purchased")
plt.title("Age vs Purchase")
plt.show()

plt.figure()
plt.scatter(df["EstimatedSalary"], df["Purchased"])
plt.xlabel("Estimated Salary")
plt.ylabel("Purchased")
plt.title("Salary vs Purchase")
plt.show()

"""We can say from the graph :-Purchased contains only 0 and 1

No label noise like 2, -1...

Outlier Check
"""

plt.boxplot(df["Age"])
plt.title("Age Outlier Detection")
plt.show()

"""boxplot shows that the Age feature does not contain any outliers based on the IQR method. All values fall within the acceptable range, indicating that Age is a clean and reliable feature for modeling"""

plt.boxplot(df["EstimatedSalary"])
plt.title("EstimatedSalary Outlier Detection")
plt.show()

"""No Outlier

Training The Model
"""

X = df[["Gender", "Age", "EstimatedSalary"]]
y = df["Purchased"]

skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)

log_model = Pipeline([
    ("scaler", StandardScaler()),
    ("logreg", LogisticRegression())
])

cv_accuracy = cross_val_score(
    log_model,
    X,
    y,
    cv=skf,
    scoring="accuracy"
)

print("\nStratified K-Fold CV Results")
print("Fold Accuracies:", cv_accuracy)
print("Mean CV Accuracy:", cv_accuracy.mean())

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

log_model.fit(X_train, y_train)

y_pred = log_model.predict(X_test)
y_prob = log_model.predict_proba(X_test)[:, 1]

print("\n-Logistic Regression Metrics (Test Set)")
print("Accuracy :", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall   :", recall_score(y_test, y_pred))
print("F1 Score :", f1_score(y_test, y_pred))

mse = mean_squared_error(y_test, y_prob)
rmse = np.sqrt(mse)
print("MSE :", mse)
print("RMSE :", rmse)

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# ROC values
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
auc_score = roc_auc_score(y_test, y_prob)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f"ROC-AUC = {auc_score:.2f}")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve â€“ Logistic Regression")
plt.legend()
plt.show()

print("ROC-AUC Score:", auc_score)

"""Decision Boundary"""

import numpy as np
fixed_gender = 0
# Create mesh grid
age_min, age_max = X["Age"].min() - 1, X["Age"].max() + 1
sal_min, sal_max = X["EstimatedSalary"].min() - 1000, X["EstimatedSalary"].max() + 1000

age_grid, sal_grid = np.meshgrid(
    np.linspace(age_min, age_max, 200),
    np.linspace(sal_min, sal_max, 200)
)

# Prepare input for prediction
grid_data = np.c_[
    np.full(age_grid.ravel().shape, fixed_gender),
    age_grid.ravel(),
    sal_grid.ravel()
]

# Predict probabilities on grid
prob_grid = log_model.predict_proba(grid_data)[:, 1]
prob_grid = prob_grid.reshape(age_grid.shape)

# Plot decision boundary
plt.figure(figsize=(7, 6))
plt.contourf(age_grid, sal_grid, prob_grid >= 0.5, alpha=0.3)
plt.scatter(
    X_test["Age"],
    X_test["EstimatedSalary"],
    c=y_test,
    edgecolor="k"
)
plt.xlabel("Age")
plt.ylabel("Estimated Salary")
plt.title("Decision Boundary (Logistic Regression)")
plt.show()

""" visualise the results by plotting how probability varies with difference features"""

df["Purchase_Probability"] = log_model.predict_proba(X)[:, 1]

plt.figure()
plt.scatter(df["Age"], df["Purchase_Probability"])
plt.xlabel("Age")
plt.ylabel("Purchase Probability")
plt.title("Purchase Probability vs Age")
plt.show()

plt.figure()
plt.scatter(df["EstimatedSalary"], df["Purchase_Probability"])
plt.xlabel("Estimated Salary")
plt.ylabel("Purchase Probability")
plt.title("Purchase Probability vs Salary")
plt.show()

"""Age is a dominant feature over other features ,because its graph is most likely same to sigmoid

Handle Imbalance
"""

log_model_balance = Pipeline([
    ("scaler", StandardScaler()),
    ("logreg", LogisticRegression(class_weight="balanced"))#class_weight="balanced":-Reweights classes to handle imbalance
])

cv_accuracy_balance = cross_val_score(
    log_model_balance,
    X,
    y,
    cv=skf,
    scoring="accuracy"
)

log_model_balance.fit(X_train, y_train)

y_pred = log_model_balance.predict(X_test)
y_prob = log_model_balance.predict_proba(X_test)[:, 1]

print("\nLogistic Regression Metrics (Test Set)")
print("Accuracy :", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall   :", recall_score(y_test, y_pred))
print("F1 Score :", f1_score(y_test, y_pred))

mse = mean_squared_error(y_test, y_prob)
rmse = np.sqrt(mse)
print("MSE      :", mse)
print("RMSE     :", rmse)

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""Accuracy increase from 0.81 to 0.83.

"""

df["Purchase_Probability"] = log_model_balance.predict_proba(X)[:, 1]

plt.figure()
plt.scatter(df["Age"], df["Purchase_Probability"])
plt.xlabel("Age")
plt.ylabel("Purchase Probability")
plt.title("Purchase Probability vs Age")
plt.show()

plt.figure()
plt.scatter(df["EstimatedSalary"], df["Purchase_Probability"])
plt.xlabel("Estimated Salary")
plt.ylabel("Purchase Probability")
plt.title("Purchase Probability vs Salary")
plt.show()

"""KNN"""

knn_model = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier(
        n_neighbors=7,
        weights="distance"
    ))
])

knn_model.fit(X_train, y_train)

y_pred_knn = knn_model.predict(X_test)

print("\n-KNN Metrics (Test Set)")
print("Accuracy :", accuracy_score(y_test, y_pred_knn))
print("Precision:", precision_score(y_test, y_pred_knn))
print("Recall   :", recall_score(y_test, y_pred_knn))
print("F1 Score :", f1_score(y_test, y_pred_knn))


print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_knn))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_knn))

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_scores = cross_val_score(
    knn_model,
    X,
    y,
    cv=skf,
    scoring="accuracy"
)

print("\n Stratified K-Fold Cross-Validation")
print("Fold Accuracies:", cv_scores)
print("Mean CV Accuracy:", cv_scores.mean())

import numpy as np
import matplotlib.pyplot as plt

fixed_gender = 0  # Male

# Create mesh grid
age_min, age_max = X["Age"].min() - 1, X["Age"].max() + 1
sal_min, sal_max = X["EstimatedSalary"].min() - 1000, X["EstimatedSalary"].max() + 1000

age_grid, sal_grid = np.meshgrid(
    np.linspace(age_min, age_max, 300),
    np.linspace(sal_min, sal_max, 300)
)

grid_points = np.c_[
    np.full(age_grid.ravel().shape, fixed_gender),
    age_grid.ravel(),
    sal_grid.ravel()
]

# Predict class on grid
Z = knn_model.predict(grid_points)
Z = Z.reshape(age_grid.shape)

# Plot decision boundary
plt.figure(figsize=(7, 6))
plt.contourf(age_grid, sal_grid, Z, alpha=0.3)
plt.scatter(
    X_test["Age"],
    X_test["EstimatedSalary"],
    c=y_test,
    edgecolor="k"
)
plt.xlabel("Age")
plt.ylabel("Estimated Salary")
plt.title("KNN Decision Boundary (Age vs Salary)")
plt.show()

knn_prob = knn_model.predict_proba(X)[:, 1]

plt.figure(figsize=(6, 5))
plt.scatter(X["Age"], knn_prob, alpha=0.7)
plt.xlabel("Age")
plt.ylabel("Purchase Probability")
plt.title("KNN: Purchase Probability vs Age")
plt.show()

plt.figure(figsize=(6, 5))
plt.scatter(X["EstimatedSalary"], knn_prob, alpha=0.7)
plt.xlabel("Estimated Salary")
plt.ylabel("Purchase Probability")
plt.title("KNN: Purchase Probability vs Salary")
plt.show()

import seaborn as sns

df_knn = df.copy()
df_knn["KNN_Probability"] = knn_prob

plt.figure(figsize=(5, 4))
sns.boxplot(x="Gender", y="KNN_Probability", data=df_knn)
plt.xlabel("Gender (0 = Male, 1 = Female)")
plt.ylabel("Purchase Probability")
plt.title("KNN: Probability vs Gender")
plt.show()

"""Handle Imbalance

"""

knn_model_balance = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier(n_neighbors=10,weights="distance"))
])

knn_model_balance.fit(X_train, y_train)
y_pred_knn_balance = knn_model_balance.predict(X_test)

print("\nKNN Balance ---")
print("Accuracy :", accuracy_score(y_test, y_pred_knn_balance))
print("Precision:", precision_score(y_test, y_pred_knn_balance))
print("Recall   :", recall_score(y_test, y_pred_knn_balance))
print("F1 Score :", f1_score(y_test, y_pred_knn_balance))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn_balance))

"""K-clustering

"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

df = pd.read_csv("Purchase_Logistic.csv")


df["Gender"] = df["Gender"].map({"Male": 0, "Female": 1})

X_cluster = df[["Age", "EstimatedSalary"]]


# Scale Features (IMPORTANT)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)


#  Apply K-Means Clustering

kmeans = KMeans(n_clusters=2, random_state=42)
df["Cluster"] = kmeans.fit_predict(X_scaled)


#Analyze Clusters

cluster_summary = df.groupby("Cluster").agg({
    "Age": "mean",
    "EstimatedSalary": "mean",
    "Purchased": "mean",
    "Cluster": "count"
})

cluster_summary.rename(columns={"Cluster": "Count"}, inplace=True)

print("\n--- Cluster Summary ---")
print(cluster_summary)


# Create CUSTOM CUSTOMER TYPE (Custom DataType)

# Identify which cluster is high-value
high_value_cluster = cluster_summary["Purchased"].idxmax()

def assign_customer_type(cluster):
    if cluster == high_value_cluster:
        return "High-Value Customer"
    else:
        return "Low-Value Customer"

df["Customer_Type"] = df["Cluster"].apply(assign_customer_type)

# Final Dataset with Custom Datatype

print("\n--- Sample with Custom Customer Type ---")
print(df[["Age", "EstimatedSalary", "Purchased", "Cluster", "Customer_Type"]].head())


#  Visualize Clusters with Custom Labels

plt.figure(figsize=(7, 6))
colors = df["Customer_Type"].map({
    "Low-Value Customer": "blue",
    "High-Value Customer": "red"
})

plt.scatter(
    df["Age"],
    df["EstimatedSalary"],
    c=colors,
    alpha=0.7
)

plt.xlabel("Age")
plt.ylabel("Estimated Salary")
plt.title("Customer Segmentation using K-Means")
plt.show()